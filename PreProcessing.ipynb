{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "mkOrOmgPXjSu",
        "VY-MB7dqX8gP"
      ],
      "authorship_tag": "ABX9TyMITZ30HQ8xJf3KDgM6dc9u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/majidiali1/machine-learning/blob/main/PreProcessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Cleaning**"
      ],
      "metadata": {
        "id": "mkOrOmgPXjSu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def fill_missing_values(df, method='drop', value=None):\n",
        "    \"\"\" methods: Method to handle missing values ('drop', 'constant', 'mean', 'median', 'mode', 'ffill', 'bfill') \"\"\"\n",
        "\n",
        "    nNAs = df.isnull().any(axis=1).sum()\n",
        "    nT = len(df.index)\n",
        "    pNAs = int(nNAs/nT*100)\n",
        "    print(f'{nNAs} samples ({pNAs})% include NA values.')\n",
        "\n",
        "    df_filled = df.copy()\n",
        "\n",
        "    if method == 'drop':\n",
        "        df_filled.dropna(inplace=True)\n",
        "    elif method == 'constant':\n",
        "        if value is None:\n",
        "            raise ValueError(\"For method='constant', a value must be provided.\")\n",
        "        df_filled.fillna(value, inplace=True)\n",
        "    elif method == 'mean':\n",
        "        df_filled.fillna(df.mean(), inplace=True)\n",
        "    elif method == 'median':\n",
        "        df_filled.fillna(df.median(), inplace=True)\n",
        "    elif method == 'mode':\n",
        "        # Mode can return multiple values per column, use the first one\n",
        "        for column in df_filled.columns:\n",
        "            df_filled[column].fillna(df_filled[column].mode()[0], inplace=True)\n",
        "    elif method == 'ffill':\n",
        "        df_filled.fillna(method='ffill', inplace=True)\n",
        "    elif method == 'bfill':\n",
        "        df_filled.fillna(method='bfill', inplace=True)\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported method provided.\")\n",
        "\n",
        "    return df_filled\n",
        "\n",
        "# Example usage:\n",
        "# Create a sample DataFrame with missing values\n",
        "data = {\n",
        "    'A': [1, np.nan, 3, 4, 5],\n",
        "    'B': [np.nan, 2, 3, np.nan, 5],\n",
        "    'C': [1, 2, np.nan, 4, 5]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Fill missing values using a specific method, e.g., 'mean'\n",
        "df_filled = fill_missing_values(df, method='mean')\n",
        "print(df_filled)\n",
        "\n"
      ],
      "metadata": {
        "id": "T6rR6qoQXmyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Smooth Noise Data**"
      ],
      "metadata": {
        "id": "VY-MB7dqX8gP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def smooth_data(df, column_name, method='moving_average', window_size=3, alpha=0.3):\n",
        "    \"\"\" methods: The smoothing method ('moving_average' or 'exponential_moving_average') \"\"\"\n",
        "    if method == 'moving_average':\n",
        "        return df[column_name].rolling(window=window_size, min_periods=1, center=True).mean()\n",
        "    elif method == 'exponential_moving_average':\n",
        "        return df[column_name].ewm(alpha=alpha, adjust=False).mean()\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported smoothing method provided.\")\n",
        "\n",
        "# Example usage\n",
        "# Create a sample DataFrame\n",
        "data = {\n",
        "    'time': range(1, 11),\n",
        "    'value': [2, 3, 4, 15, 6, 7, 67, 5, 4, 5]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Apply smoothing\n",
        "smoothed_series_moving_average = smooth_data(df, 'value', method='moving_average', window_size=3)\n",
        "smoothed_series_exponential = smooth_data(df, 'value', method='exponential_moving_average', alpha=0.3)\n",
        "\n",
        "# Print or plot the results\n",
        "print(\"Moving Average:\\n\", smoothed_series_moving_average)\n",
        "print(\"\\nExponential Moving Average:\\n\", smoothed_series_exponential)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wG22N5oYA_u",
        "outputId": "02aeefe2-26c3-4104-adf5-49aed191ee9e"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moving Average:\n",
            " 0     2.500000\n",
            "1     3.000000\n",
            "2     7.333333\n",
            "3     8.333333\n",
            "4     9.333333\n",
            "5    26.666667\n",
            "6    26.333333\n",
            "7    25.333333\n",
            "8     4.666667\n",
            "9     4.500000\n",
            "Name: value, dtype: float64\n",
            "\n",
            "Exponential Moving Average:\n",
            " 0     2.000000\n",
            "1     2.300000\n",
            "2     2.810000\n",
            "3     6.467000\n",
            "4     6.326900\n",
            "5     6.528830\n",
            "6    24.670181\n",
            "7    18.769127\n",
            "8    14.338389\n",
            "9    11.536872\n",
            "Name: value, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Detect Outliers**"
      ],
      "metadata": {
        "id": "RgU0jNEnYgOk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "def detect_and_remove_outliers(df, column_name, method='iqr'):\n",
        "    \"\"\"\n",
        "    Detects and removes outliers in a specified column of a pandas DataFrame using the specified method.\n",
        "    Prints the percentage of outlier samples before removing them.\n",
        "\n",
        "    Parameters:\n",
        "    - method: The method for detecting outliers:\n",
        "        - 'iqr': Uses the Interquartile Range (IQR) for outlier detection.\n",
        "            IQR is calculated as Q3 - Q1, where Q1 and Q3 are the 25th and 75th percentiles, respectively.\n",
        "            Outliers are defined as observations that fall below Q1 - 1.5*IQR or above Q3 + 1.5*IQR.\n",
        "        - 'z_score': Uses Z-scores for outlier detection.\n",
        "            Z-score of an observation is calculated as (X - μ) / σ, where X is the observation, μ is the mean,\n",
        "            and σ is the standard deviation of the dataset.\n",
        "            An observation is considered an outlier if its Z-score is greater than 3 or less than -3, indicating\n",
        "            it is more than 3 standard deviations away from the mean.\n",
        "\n",
        "    Returns:\n",
        "    - DataFrame without outliers.\n",
        "    \"\"\"\n",
        "    if method == 'iqr':\n",
        "        Q1 = df[column_name].quantile(0.25)\n",
        "        Q3 = df[column_name].quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "\n",
        "        lower_bound = Q1 - 1.5 * IQR\n",
        "        upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "        outliers = (df[column_name] < lower_bound) | (df[column_name] > upper_bound)\n",
        "    elif method == 'z_score':\n",
        "        z = np.abs(stats.zscore(df[column_name]))\n",
        "        outliers = z > 1\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported method provided.\")\n",
        "\n",
        "    # Calculate and print the percentage of outliers\n",
        "    outlier_percentage = 100 * outliers.sum() / len(df)\n",
        "    print(f\"Percentage of outlier samples: {outlier_percentage:.2f}%\")\n",
        "\n",
        "    # Remove outliers\n",
        "    df_cleaned = df[~outliers]\n",
        "\n",
        "    return df_cleaned\n",
        "\n",
        "# Example usage\n",
        "data = {\n",
        "    'time': range(1, 11),\n",
        "    'value': [2, 3, 4, 5, 6, 1000, 60, 5, 4, 5]  # Assuming 100 is an outlier\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Detect and remove outliers using IQR\n",
        "df_cleaned_iqr = detect_and_remove_outliers(df, 'value', method='iqr')\n",
        "print(\"\\nDataFrame after removing outliers using IQR:\\n\", df_cleaned_iqr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GegGSxZfYrQT",
        "outputId": "0f832044-14f1-4c2b-d336-3a47399ad667"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Percentage of outlier samples: 20.00%\n",
            "\n",
            "DataFrame after removing outliers using IQR:\n",
            "    time  value\n",
            "0     1      2\n",
            "1     2      3\n",
            "2     3      4\n",
            "3     4      5\n",
            "4     5      6\n",
            "7     8      5\n",
            "8     9      4\n",
            "9    10      5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Reduction: PCA**"
      ],
      "metadata": {
        "id": "r8dc7hKcx5D3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.datasets import make_blobs\n",
        "\n",
        "# Generate a sample dataset\n",
        "X, y = make_blobs(n_samples=400, centers=5, n_features=15, random_state=42)\n",
        "\n",
        "# Perform PCA\n",
        "pca = PCA(n_components=5)  # Reduce to 2 dimensions\n",
        "X_pca = pca.fit_transform(X)\n",
        "\n",
        "def pca_inverse_transform(new_sample_pca):\n",
        "  return pca.inverse_transform(new_sample_pca)\n",
        "\n",
        "def plot_pca_results(pca, X_pca, method = 'scatter-plot', labels=None):\n",
        "  if method == 'scatter-plot':\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, edgecolor='k', cmap='viridis')\n",
        "    plt.xlabel('First principal component')\n",
        "    plt.ylabel('Second principal component')\n",
        "    plt.title('PCA - First two principal components')\n",
        "    plt.colorbar(label='Cluster label')\n",
        "    plt.show()\n",
        "  elif method == 'explained-variance':\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.bar(range(1, pca.n_components_ + 1), pca.explained_variance_ratio_)\n",
        "    plt.xlabel('Principal Component')\n",
        "    plt.ylabel('Variance Explained')\n",
        "    plt.title('PCA Results: Explained Variance')\n",
        "    plt.show()\n",
        "  elif method == 'bi-plot':\n",
        "    coeff = np.transpose(pca.components_[0:2, :])\n",
        "    xs = X_pca[:,0]\n",
        "    ys = X_pca[:,1]\n",
        "    n = coeff.shape[0]\n",
        "    scalex = 1.0/(xs.max() - xs.min())\n",
        "    scaley = 1.0/(ys.max() - ys.min())\n",
        "\n",
        "    plt.scatter(xs * scalex, ys * scaley, c='r')\n",
        "    for i in range(n):\n",
        "        plt.arrow(0, 0, coeff[i,0], coeff[i,1], color='b', alpha=0.5)\n",
        "        if labels is None:\n",
        "            plt.text(coeff[i,0]*1.15, coeff[i,1]*1.15, \"Var\"+str(i+1), color='g', ha='center', va='center')\n",
        "        else:\n",
        "            plt.text(coeff[i,0]*1.15, coeff[i,1]*1.15, labels[i], color='g', ha='center', va='center')\n",
        "    plt.xlabel(\"PC{}\".format(1))\n",
        "    plt.ylabel(\"PC{}\".format(2))\n",
        "    plt.grid()\n",
        "\n"
      ],
      "metadata": {
        "id": "pE52fcR-x_o2"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Data Reduction: Feature subset selection**"
      ],
      "metadata": {
        "id": "VoFB1Vfu3yhw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import matplotlib.pyplot as plt\n",
        "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
        "\n",
        "def feature_subset_selection(X, y, method='forward'):\n",
        "    \"\"\"\n",
        "    Performs feature subset selection using forward selection or backward elimination.\n",
        "\n",
        "    Parameters:\n",
        "    - X: Feature matrix.\n",
        "    - y: Target vector.\n",
        "    - method: 'forward' for forward selection, 'backward' for backward elimination.\n",
        "\n",
        "    Returns:\n",
        "    - A tuple containing the list of selected features and the performance metric for each number of features.\n",
        "    \"\"\"\n",
        "    # Split the dataset into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "    # Define the model\n",
        "    model = LogisticRegression(max_iter=1000)\n",
        "\n",
        "    # Define the Sequential Feature Selector\n",
        "    sfs = SFS(model,\n",
        "              k_features='best',\n",
        "              forward=(method == 'forward'),\n",
        "              scoring='accuracy',\n",
        "              cv=5)\n",
        "\n",
        "    # Fit SFS\n",
        "    sfs.fit(X_train, y_train)\n",
        "\n",
        "    # Get the performance metric for each number of features\n",
        "    metric_per_feature_count = [sfs.subsets_[k]['avg_score'] for k in sfs.subsets_]\n",
        "\n",
        "    # Plot the performance metric as a function of the number of features\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(range(1, len(sfs.subsets_)+1), metric_per_feature_count, marker='o')\n",
        "    plt.title(f'Feature Selection using {method.capitalize()} Selection')\n",
        "    plt.xlabel('Number of Features')\n",
        "    plt.ylabel('Cross-Validation Accuracy')\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    # Return the selected feature indices and the performance metric\n",
        "    return sfs.k_feature_idx_, metric_per_feature_count\n"
      ],
      "metadata": {
        "id": "NBT2xwPL4EK4"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Standardization**"
      ],
      "metadata": {
        "id": "47sEkiml5TtJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "import pandas as pd\n",
        "\n",
        "def standardize_data(df, columns, method='z_score'):\n",
        "    \"\"\"\n",
        "    Standardizes the data in the specified columns of a pandas DataFrame.\n",
        "\n",
        "    Parameters:\n",
        "    - df: Pandas DataFrame containing the data.\n",
        "    - columns: List of column names to be standardized.\n",
        "    - method: Method used for standardization ('z_score' or 'min_max').\n",
        "\n",
        "    Returns:\n",
        "    - A DataFrame with the specified columns standardized.\n",
        "    \"\"\"\n",
        "    df_standardized = df.copy()\n",
        "    scaler = None\n",
        "\n",
        "    if method == 'z_score':\n",
        "        scaler = StandardScaler()\n",
        "    elif method == 'min_max':\n",
        "        scaler = MinMaxScaler()\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported method provided. Choose 'z_score' or 'min_max'.\")\n",
        "\n",
        "    df_standardized[columns] = scaler.fit_transform(df_standardized[columns])\n",
        "\n",
        "    return df_standardized\n",
        "\n",
        "# Example usage:\n",
        "data = {\n",
        "    'Feature1': [1, 2, 3, 4, 5],\n",
        "    'Feature2': [10, 20, 30, 40, 50],\n",
        "    'NonStandardFeature': [100, 200, 300, 400, 500]  # Assume we don't want to standardize this\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Standardize the data using Z-score normalization\n",
        "df_z_score = standardize_data(df, ['Feature1', 'Feature2'], method='z_score')\n",
        "print(\"Z-Score Standardization:\\n\", df_z_score)\n",
        "\n",
        "# Standardize the data using Min-Max scaling\n",
        "df_min_max = standardize_data(df, ['Feature1', 'Feature2'], method='min_max')\n",
        "print(\"\\nMin-Max Scaling:\\n\", df_min_max)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OhxOF11I5atj",
        "outputId": "3c144e6b-0267-4519-ced0-1b478dc763bf"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Z-Score Standardization:\n",
            "    Feature1  Feature2  NonStandardFeature\n",
            "0 -1.414214 -1.414214                 100\n",
            "1 -0.707107 -0.707107                 200\n",
            "2  0.000000  0.000000                 300\n",
            "3  0.707107  0.707107                 400\n",
            "4  1.414214  1.414214                 500\n",
            "\n",
            "Min-Max Scaling:\n",
            "    Feature1  Feature2  NonStandardFeature\n",
            "0      0.00      0.00                 100\n",
            "1      0.25      0.25                 200\n",
            "2      0.50      0.50                 300\n",
            "3      0.75      0.75                 400\n",
            "4      1.00      1.00                 500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def preprocess_dataframe(df):\n",
        "    # Make a copy of the DataFrame to avoid changing the original data\n",
        "    processed_df = df.copy()\n",
        "\n",
        "    # Process each column based on its dtype\n",
        "    for column in processed_df.columns:\n",
        "        # Detect datetime columns and extract components\n",
        "        if pd.api.types.is_datetime64_any_dtype(processed_df[column]):\n",
        "            processed_df[f'{column}_Year'] = processed_df[column].dt.year\n",
        "            processed_df[f'{column}_Month'] = processed_df[column].dt.month\n",
        "            processed_df[f'{column}_Day'] = processed_df[column].dt.day\n",
        "            # Optionally, drop the original datetime column if no longer needed\n",
        "            # processed_df.drop(column, axis=1, inplace=True)\n",
        "\n",
        "        # Detect categorical columns and apply one-hot encoding\n",
        "        elif pd.api.types.is_categorical_dtype(processed_df[column]) or processed_df[column].dtype == 'object':\n",
        "            processed_df = pd.get_dummies(processed_df, columns=[column], drop_first=True)\n",
        "\n",
        "    # Standardize numerical columns\n",
        "    # Exclude any newly created columns (Year, Month, Day, and dummies) from standardization\n",
        "    num_cols = processed_df.select_dtypes(include=['float64', 'int64']).columns\n",
        "    scaler = StandardScaler()\n",
        "    processed_df[num_cols] = scaler.fit_transform(processed_df[num_cols])\n",
        "\n",
        "    return processed_df\n",
        "\n",
        "# Example usage\n",
        "data = {\n",
        "    'Date': pd.to_datetime(['2020-01-01', '2020-01-02', '2020-01-03', '2020-01-04', '2020-01-05']),\n",
        "    'Category': ['A', 'B', 'A', 'C', 'B'],\n",
        "    'Value': [100, 150, 120, 130, 110]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "# Convert 'Category' to a categorical data type for demonstration purposes\n",
        "df['Category'] = df['Category'].astype('category')\n",
        "\n",
        "# Preprocess the DataFrame\n",
        "processed_df = preprocess_dataframe(df)\n",
        "\n"
      ],
      "metadata": {
        "id": "k4rZud-T54Lt"
      },
      "execution_count": 30,
      "outputs": []
    }
  ]
}